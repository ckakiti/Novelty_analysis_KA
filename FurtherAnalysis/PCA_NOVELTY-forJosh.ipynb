{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as LA\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def pca_on_np_array(nparray):\n",
    "    '''rows are members (mice), columns are variables (syllables)'''\n",
    "    nparray -= np.mean(nparray, axis = 0)  \n",
    "    #calculate covariance matrix\n",
    "    cov = np.cov(nparray, rowvar = False)\n",
    "    #get eigenvectors and eigenvalues\n",
    "    evals , evecs = LA.eigh(cov)\n",
    "\n",
    "    #formatting and SORT by eigenvalues (aka sort by how much variance each eigen vector explains)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evecs = evecs[:,idx]\n",
    "    evals = evals[idx]\n",
    "\n",
    "    #this is your data represented as PCS \n",
    "    nparray_represented_as_pcs = np.dot(nparray, evecs)\n",
    "    \n",
    "    return nparray_represented_as_pcs,evecs,evals\n",
    "\n",
    "def var_explained(evals): #UNUSED FXN\n",
    "    var_explained=evals/sum(evals)\n",
    "    return var_explained\n",
    "\n",
    "def pc_dist(nparray1,nparray2):\n",
    "    '''\n",
    "    calculate the distance between two groups and within each group\n",
    "    two groups should have the same number of variables columns\n",
    "    '''\n",
    "    \n",
    "    # first do pca on COMBINED data\n",
    "    a,b,c=pca_on_np_array(np.concatenate((nparray1,nparray2),axis=0))\n",
    "    #a=data represented as pc scores rather than variable (syllable) values; rows=mice, columns=PCs\n",
    "    #b=eigenvectors\n",
    "    #c=eigenvalue corresponding to each eigen vector (use this to calculate variance explained by each pc)\n",
    "    \n",
    "    #within array 1 dist\n",
    "    dist1=[]\n",
    "    for combos in list(itertools.combinations(a[0:len(nparray1)], 2)): #goes through pairs of mice (e.g. 1vs2,1vs3,1vs4,etc)\n",
    "        #calculate difference by finding difference in each pc and doing squareroot of sum of squares\n",
    "        dist1.append(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "        #print(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "    \n",
    "   \n",
    "    #within array 2 dist\n",
    "    dist2=[]\n",
    "    for combos in list(itertools.combinations(a[len(nparray1):len(a)], 2)):\n",
    "        #calculate difference by finding difference in each pc and doing squareroot of sum of squares\n",
    "        dist2.append(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "        #print(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "        \n",
    "        \n",
    "    #between array 1 and 2 dist\n",
    "    betweendist=[]\n",
    "    for combos in list(itertools.product(nparray1, nparray2)):\n",
    "        betweendist.append(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "        \n",
    "    return betweendist,dist1,dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# processing my data\n",
    "n_mice=9 #12 #9 ########################\n",
    "n_syllables=14\n",
    "print(n_mice)\n",
    "\n",
    "#STatiStiCal SIGniFicAncE\n",
    "#the cutoff should be interpreted as: while all syllables contribute to the distance between groups mathematically,\n",
    "#only (cutoff-1)% of unintended (randomly walking) syllables should fall above the line.\n",
    "#this is not the fdr! In all examples, we draw from a random uniform distribution, however, \n",
    "#biologial variables are often drawn from normal or exponential distributions\n",
    "cutoff=.95\n",
    "compounds=int(np.ceil(np.log2(1/(1-cutoff))))\n",
    "comp=[]\n",
    "\n",
    "for i in range(compounds):\n",
    "    comp.append((1.0/n_syllables)/(2.0*(i+1.0)))\n",
    "cutoff=(1.0/n_syllables) + sum(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 14)\n",
      "(9, 14)\n",
      "/Users/cakiti/Dropbox (Uchida Lab)/Korleki Akiti/Behavior/Standard setup/CombineAnalysis/Dataset_20190723\n"
     ]
    }
   ],
   "source": [
    "curr_dataset = 'Dataset_20190723' #Dataset_20190723 #Dataset_20191007\n",
    "which_day    = 'N1';         #N1\n",
    "sesh_len     = '10min';      #10min #10min_noB\n",
    "event_type   = 'poke_syl10'; #poke #poke_syl10\n",
    "group1_name  = 'stim_noB';   #stim\n",
    "group2_name  = 'cont';       #cont\n",
    "\n",
    "group1_file_name = curr_dataset+'/'+curr_dataset+'_sylExpr_'+which_day+'_'+event_type+'_'+group1_name+'.csv'\n",
    "group2_file_name = curr_dataset+'/'+curr_dataset+'_sylExpr_'+which_day+'_'+event_type+'_'+group2_name+'.csv'\n",
    "#group1_file_name = curr_dataset+'/'+curr_dataset+'_sylExpr_'+which_day+'_'+group1_name+'_'+sesh_len+'.csv'\n",
    "#group2_file_name = curr_dataset+'/'+curr_dataset+'_sylExpr_'+which_day+'_'+group2_name+'_'+sesh_len+'.csv'\n",
    "file_path = '/Users/cakiti/Dropbox (Uchida Lab)/Korleki Akiti/Behavior/Standard setup/CombineAnalysis/'\n",
    "\n",
    "csv_group1 = pd.read_csv(os.path.join(file_path,group1_file_name), header = None)\n",
    "csv_group2 = pd.read_csv(os.path.join(file_path,group2_file_name), header = None)\n",
    "csv_group1 = np.float64(csv_group1)\n",
    "csv_group2 = np.float64(csv_group2)\n",
    "\n",
    "print(csv_group1.shape)\n",
    "print(csv_group2.shape)\n",
    "\n",
    "save_where = os.path.join(file_path,curr_dataset)\n",
    "print(save_where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/cakiti/Dropbox (Uchida Lab)/Korleki Akiti/Behavior/Standard setup/CombineAnalysis/Dataset_20190723/Dataset_20190723_N1_10min_varExplained.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract PC distances array (first step in pc_dist)\n",
    "nparray = np.concatenate((csv_group1,csv_group2),axis=0)\n",
    "a,b,c = pca_on_np_array(nparray)\n",
    "display(a.shape)\n",
    "#a=data represented as pc scores rather than variable (syllable) values; rows=mice, columns=PCs\n",
    "#b=eigenvectors\n",
    "#c=eigenvalue corresponding to each eigen vector (use this to calculate variance explained by each pc)\n",
    "\n",
    "#save_where = os.path.join(file_path,curr_dataset)\n",
    "#np.savetxt(save_where+'/'+curr_dataset+'_'+which_day+'_'+sesh_len+'_PCSarray.txt', nparray_represented_as_pcs, delimiter=',', fmt=\"%.2f\")\n",
    "\n",
    "display(save_where+'/'+curr_dataset+'_'+which_day+'_'+sesh_len+'_varExplained.txt')\n",
    "np.savetxt(save_where+'/'+curr_dataset+'_'+which_day+'_'+sesh_len+'_varExplained.txt',var_explained(c),delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA reconstruction = PC_scores * eigenvectors^T + mean\n",
    "#     normalized reconstruction error = (|| X - X^recon ||^2) / ||X||^2 where ||X|| = frobenius norm of X\n",
    "array_mean = np.mean(nparray, axis = 0)\n",
    "#R=3\n",
    "\n",
    "recon_err_norm = []\n",
    "for R in np.arange(0,len(nparray)):\n",
    "    reconstr = np.dot(a[:,0:R],np.transpose(b[:,0:R])) + array_mean\n",
    "    recon_err = np.linalg.norm((nparray-reconstr),'fro') ** 2\n",
    "    norm = recon_err / (np.linalg.norm(nparray,'fro') ** 2)\n",
    "    recon_err_norm.append(norm)\n",
    "\n",
    "\n",
    "#np.savetxt(save_where+'/test_reconstruction_first3PCs.txt',reconstr, delimiter=',')\n",
    "#np.savetxt(save_where+'/test_inputArray.txt',nparray, delimiter=',')\n",
    "np.savetxt(save_where+'/'+curr_dataset+'_'+which_day+'_'+sesh_len+'_recon_err_norm_R=1-'+str(len(nparray))+'.txt',recon_err_norm,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 229.6,
   "position": {
    "height": "40px",
    "left": "870.6px",
    "right": "20px",
    "top": "71px",
    "width": "483.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
