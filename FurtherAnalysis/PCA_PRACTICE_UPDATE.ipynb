{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as LA\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  5. , 13. ],\n",
       "       [ 5. ,  9. , 28. ],\n",
       "       [ 4. ,  6. ,  2. ],\n",
       "       [ 5. ,  8. , 16. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.26916667,   0.26666667,   4.19166667],\n",
       "       [  0.26666667,   3.33333333,  14.66666667],\n",
       "       [  4.19166667,  14.66666667, 114.25      ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1.16307993e+02, 1.48242723e+00, 6.20799178e-02])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([9.86894575e+01, 1.25786660e+00, 5.26759447e-02])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mock data #imagine rows are syllables and columns are mice\n",
    "#change values and/or size of array to see how it effects the eigenvectors/eigenvalues \n",
    "\n",
    "#in our mock data we have 4 mice that produce 3 syllables\n",
    "mockdata = np.array([  \n",
    "        [5.1,5, 13],\n",
    "        [5,9,28],\n",
    "        [4,6,2],\n",
    "        [5,8,16],])\n",
    "display(mockdata)\n",
    "\n",
    "mocksum = np.sum(mockdata,axis=1)\n",
    "mockdata_norm = mockdata/np.transpose(np.matlib.repmat(mocksum,3,1))\n",
    "#mockdata = mockdata_norm\n",
    "#display(mockdata)\n",
    "\n",
    "#centering the data\n",
    "mockdata -= np.mean(mockdata, axis = 0)  \n",
    "#calculate covariance matrix\n",
    "cov = np.cov(mockdata, rowvar = False)\n",
    "display(cov)\n",
    "#get eigenvectors and eigenvalues\n",
    "evals , evecs = LA.eigh(cov)\n",
    "\n",
    "#formatting and SORT by eigenvalues (aka sort by how much variance each eigen vector explains)\n",
    "idx = np.argsort(evals)[::-1]\n",
    "evecs = evecs[:,idx]\n",
    "evals = evals[idx]\n",
    "\n",
    "display(evals)\n",
    "display(evals/sum(evals)*100)\n",
    "\n",
    "#this is your data represented as PCS \n",
    "mockdata_represented_as_pcs = np.dot(mockdata, evecs)\n",
    "#display(mockdata_represented_as_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.product at 0x1a21a56870>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itertools.product([1,2,3],[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through some examples\n",
    "n_mice=4\n",
    "n_syllables=3\n",
    "\n",
    "#STatiStiCal SIGniFicAncE\n",
    "#the cutoff should be interpreted as: while all syllables contribute to the distance between groups mathematically,\n",
    "#only (cutoff-1)% of unintended (randomly walking) syllables should fall above the line.\n",
    "#this is not the fdr! In all examples, we draw from a random uniform distribution, however, \n",
    "#biologial variables are often drawn from normal or exponential distributions\n",
    "cutoff=.95\n",
    "compounds=int(np.ceil(np.log2(1/(1-cutoff))))\n",
    "comp=[]\n",
    "for i in range(compounds):\n",
    "    comp.append((1/n_syllables)/(2*(i+1)))\n",
    "cutoff=(1/n_syllables) + sum(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_on_np_array(nparray):\n",
    "    '''rows are members (mice), columns are variables (syllables)'''\n",
    "    nparray -= np.mean(nparray, axis = 0)  \n",
    "    #calculate covariance matrix\n",
    "    cov = np.cov(nparray, rowvar = False)\n",
    "    #get eigenvectors and eigenvalues\n",
    "    evals , evecs = LA.eigh(cov)\n",
    "\n",
    "    #formatting and SORT by eigenvalues (aka sort by how much variance each eigen vector explains)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evecs = evecs[:,idx]\n",
    "    evals = evals[idx]\n",
    "\n",
    "    #this is your data represented as PCS \n",
    "    nparray_represented_as_pcs = np.dot(nparray, evecs)\n",
    "    \n",
    "    return nparray_represented_as_pcs,evecs,evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_explained(evals):\n",
    "    var_explained=evals/sum(evals)\n",
    "    return var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_dist(nparray1,nparray2):\n",
    "    '''\n",
    "    calculate the distance between two groups and within each group\n",
    "    two groups should have the same number of variables columns\n",
    "    '''\n",
    "    \n",
    "    # first do pca on COMBINED data\n",
    "    a,b,c=pca_on_np_array(np.concatenate((nparray1,nparray2),axis=0))\n",
    "    #a=data represented as pc scores rather than variable (syllable) values\n",
    "    #b=eigenvectors\n",
    "    #c=eigenvalue corresponding to each eigen vector (use this to calculate variance explained by each pc)\n",
    "    \n",
    "    #within array 1 dist\n",
    "    dist1=[]\n",
    "    for combos in list(itertools.combinations(a[0:len(nparray1)], 2)):\n",
    "        #calculate difference by finding difference in each pc and doing squareroot of sum of squares\n",
    "        dist1.append(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "        #print(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "    \n",
    "   \n",
    "    #within array 2 dist\n",
    "    dist2=[]\n",
    "    for combos in list(itertools.combinations(a[len(nparray1):len(a)], 2)):\n",
    "        #calculate difference by finding difference in each pc and doing squareroot of sum of squares\n",
    "        dist2.append(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "        #print(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "        \n",
    "        \n",
    "    #between array 1 and 2 dist\n",
    "    betweendist=[]\n",
    "    for combos in list(itertools.product(nparray1, nparray2)):\n",
    "        betweendist.append(sum(np.square(np.subtract(combos[0],combos[1])))**.5)\n",
    "        \n",
    "    return betweendist,dist1,dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contribution_of_each_syllable_to_distance(nparray1,nparray2):\n",
    "    pcs,eigenvectors,eigenvalues=pca_on_np_array(np.concatenate((nparray1,nparray2),axis=0))\n",
    "\n",
    "    n_mice1=len(nparray1)\n",
    "    n_mice2=len(nparray2)\n",
    "    n_syllables=len(nparray1[0])\n",
    "\n",
    "    #get the mean pc scores for each group\n",
    "    group1_mean_pcs=np.mean(pcs[0:(n_mice1-1)],axis=0)\n",
    "    group2_mean_pcs=np.mean(pcs[n_mice1:((n_mice1+n_mice2)-1)],axis=0)\n",
    "    #calculate the absolute pairwise difference between pcs and divide by the sum \n",
    "    difference=np.abs(np.subtract(group1_mean_pcs,group2_mean_pcs))\n",
    "    square_difference=np.square(difference)\n",
    "    sum_of_square_difference=sum(square_difference)\n",
    "    weighted_difference=square_difference/sum_of_square_difference\n",
    "\n",
    "    #calculate how much each syllable contributes to each eigenvector and ultimately the distance between groups\n",
    "    contribution_of_each_syllable=np.zeros((n_syllables,n_syllables),dtype=float)\n",
    "    for i in range(n_syllables):\n",
    "        pc_i_contribution_to_all_syllables=weighted_difference[i]*np.square(eigenvectors[:,i])*((eigenvalues[i]**2/sum(eigenvalues**2)))\n",
    "        contribution_of_each_syllable[i,:]=pc_i_contribution_to_all_syllables\n",
    "    \n",
    "    contribution_of_each_syllable=sum(contribution_of_each_syllable)/sum(sum(contribution_of_each_syllable))\n",
    "    \n",
    "    return contribution_of_each_syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contribution_of_each_syllable_to_distance_raw(nparray1,nparray2):\n",
    "    '''distance can be calculated without doing pca'''\n",
    "    \n",
    "    n_mice1=len(nparray1)\n",
    "    n_mice2=len(nparray2)\n",
    "    n_syllables=len(nparray1[0])\n",
    "\n",
    "    #get the mean pc scores for each group\n",
    "    group1_mean=np.mean(nparray1,axis=0)\n",
    "    group2_mean=np.mean(nparray2,axis=0)\n",
    "\n",
    "    #calculate the absolute pairwise difference between pcs and divide by the sum \n",
    "    difference=np.abs(np.subtract(group1_mean,group2_mean))\n",
    "    square_difference=np.square(difference)\n",
    "    sum_of_difference=sum(square_difference)\n",
    "    weighted_difference=square_difference/sum_of_difference\n",
    "\n",
    "    return weighted_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example1: two groups, each have n_mice mice and produce n_syllables syllables\n",
    "#in this example group2 (red) uses it's syllables about twice as much as group 1(+.5) (artificially boosted average)\n",
    "example1_group1=np.random.rand(n_mice,n_syllables)\n",
    "example1_group2=np.random.rand(n_mice,n_syllables)+.5\n",
    "example1_between,example1_within1,example1_within2=pc_dist(example1_group1,example1_group2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(example1_between, color='purple')\n",
    "sns.distplot(example1_within1, color='blue')\n",
    "sns.distplot(example1_within2, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example2: two groups, each have 20 mice and produce 10 syllables\n",
    "#in this example group2 (red) uses the first syllable ~ three times as much (~+1) as group 1 does\n",
    "example2_group1=np.random.rand(n_mice,n_syllables)\n",
    "example2_group2=np.random.rand(n_mice,n_syllables)\n",
    "example2_group2[:,2]+=1.5\n",
    "example2_between,example2_within1,example2_within2=pc_dist(example2_group1,example2_group2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots()\n",
    "sns.distplot(example2_between, color='purple',label='distance between groups')\n",
    "sns.distplot(example2_within1, color='blue',label='distance within group 1')\n",
    "sns.distplot(example2_within2, color='red',label='distance within group 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: two groups' variables (syllables) are drawn from same distribution\n",
    "example3_group1=np.random.rand(n_mice,n_syllables)\n",
    "example3_group2=np.random.rand(n_mice,n_syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,3)\n",
    "f.set_size_inches(24,4)\n",
    "\n",
    "example3_between,example3_within1,example3_within2=pc_dist(example3_group1,example3_group2)\n",
    "sns.distplot(example3_between, color='purple',label=['distance between groups'],ax=axes[0])\n",
    "sns.distplot(example3_within1, color='blue',label=['distance within group 1'],ax=axes[0])\n",
    "sns.distplot(example3_within2, color='red',label=['distance within group 2'],ax=axes[0])\n",
    "axes[0].set_title('pc distance')\n",
    "axes[0].legend()\n",
    "\n",
    "example3=contribution_of_each_syllable_to_distance_raw(example3_group1,example3_group2)\n",
    "sns.barplot(x=np.array(range(n_syllables)),y=example3,ax=axes[1])\n",
    "axes[1].set_title('contribution of variable to distance')\n",
    "axes[1].axhline(cutoff, ls='--')\n",
    "\n",
    "example3=contribution_of_each_syllable_to_distance(example3_group1,example3_group2)\n",
    "sns.barplot(x=np.array(range(n_syllables)),y=example3,ax=axes[2])\n",
    "axes[2].set_title('contribution of variable to distance after pca')\n",
    "axes[2].axhline(cutoff, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: variables (syllables) 2 and 5 are artifically boosted in group 1 vs group 2\n",
    "example3_group1=np.random.rand(n_mice,n_syllables)\n",
    "example3_group2=np.random.rand(n_mice,n_syllables)\n",
    "example3_group2[:,2]+=.5\n",
    "example3_group2[:,5]+=.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,3)\n",
    "f.set_size_inches(24,4)\n",
    "\n",
    "example3_between,example3_within1,example3_within2=pc_dist(example3_group1,example3_group2)\n",
    "sns.distplot(example3_between, color='purple',label=['distance between groups'],ax=axes[0])\n",
    "sns.distplot(example3_within1, color='blue',label=['distance within group 1'],ax=axes[0])\n",
    "sns.distplot(example3_within2, color='red',label=['distance within group 2'],ax=axes[0])\n",
    "axes[0].set_title('pc distance')\n",
    "axes[0].legend()\n",
    "\n",
    "example3=contribution_of_each_syllable_to_distance_raw(example3_group1,example3_group2)\n",
    "sns.barplot(x=np.array(range(n_syllables)),y=example3,ax=axes[1])\n",
    "axes[1].set_title('contribution of variable to distance')\n",
    "axes[1].axhline(cutoff, ls='--')\n",
    "\n",
    "example3=contribution_of_each_syllable_to_distance(example3_group1,example3_group2)\n",
    "sns.barplot(x=np.array(range(n_syllables)),y=example3,ax=axes[2])\n",
    "axes[2].set_title('contribution of variable to distance after pca')\n",
    "axes[2].axhline(cutoff, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: variables (syllables) 5 and 9 are correlated in group 1 and 2, and variables 5 and 1 are artificially boosted in group 2\n",
    "example3_group1=np.random.rand(n_mice,n_syllables)\n",
    "example3_group2=np.random.rand(n_mice,n_syllables)\n",
    "example3_group1[:,9]=example3_group1[:,5]*1.1 #variable 9 is 110% of variable 5\n",
    "example3_group2[:,9]=example3_group2[:,5]*1.1 \n",
    "example3_group2[:,5]+=.6\n",
    "example3_group2[:,1]+=.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,3)\n",
    "f.set_size_inches(24,4)\n",
    "\n",
    "example3_between,example3_within1,example3_within2=pc_dist(example3_group1,example3_group2)\n",
    "sns.distplot(example3_between, color='purple',label=['distance between groups'],ax=axes[0])\n",
    "sns.distplot(example3_within1, color='blue',label=['distance within group 1'],ax=axes[0])\n",
    "sns.distplot(example3_within2, color='red',label=['distance within group 2'],ax=axes[0])\n",
    "axes[0].set_title('pc distance')\n",
    "axes[0].legend()\n",
    "\n",
    "example3=contribution_of_each_syllable_to_distance_raw(example3_group1,example3_group2)\n",
    "sns.barplot(x=np.array(range(n_syllables)),y=example3,ax=axes[1])\n",
    "axes[1].set_title('contribution of variable to distance')\n",
    "axes[1].axhline(cutoff, ls='--')\n",
    "\n",
    "example3=contribution_of_each_syllable_to_distance(example3_group1,example3_group2)\n",
    "sns.barplot(x=np.array(range(n_syllables)),y=example3,ax=axes[2])\n",
    "axes[2].set_title('contribution of variable to distance after pca')\n",
    "axes[2].axhline(cutoff, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: all variables (syllables) are artificially boosted in group 2\n",
    "example3_group1=np.random.rand(n_mice,n_syllables)\n",
    "example3_group2=np.random.rand(n_mice,n_syllables)+.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,3)\n",
    "f.set_size_inches(24,4)\n",
    "\n",
    "example3_between,example3_within1,example3_within2=pc_dist(example3_group1,example3_group2)\n",
    "sns.distplot(example3_between, color='purple',label=['distance between groups'],ax=axes[0])\n",
    "sns.distplot(example3_within1, color='blue',label=['distance within group 1'],ax=axes[0])\n",
    "sns.distplot(example3_within2, color='red',label=['distance within group 2'],ax=axes[0])\n",
    "axes[0].set_title('pc distance')\n",
    "axes[0].legend()\n",
    "\n",
    "example3=contribution_of_each_syllable_to_distance_raw(example3_group1,example3_group2)\n",
    "sns.barplot(x=np.array(range(n_syllables)),y=example3,ax=axes[1])\n",
    "axes[1].set_title('contribution of variable to distance')\n",
    "axes[1].axhline(cutoff, ls='--')\n",
    "\n",
    "example3=contribution_of_each_syllable_to_distance(example3_group1,example3_group2)\n",
    "sns.barplot(x=np.array(range(n_syllables)),y=example3,ax=axes[2])\n",
    "axes[2].set_title('contribution of variable to distance after pca')\n",
    "axes[2].axhline(cutoff, ls='--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
